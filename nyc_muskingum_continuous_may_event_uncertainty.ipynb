{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47343f9-ecea-45d9-85ff-7b898ed5cb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import scipy.integrate\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from IPython.display import clear_output, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de093b-a5ec-4b73-8bc0-d9b23ba0e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "def muskingum_matrix(startnodes, endnodes, alpha, beta, chi, gamma, indegree):\n",
    "    m = startnodes.size\n",
    "    n = endnodes.size\n",
    "    indegree_t = indegree.copy()\n",
    "\n",
    "    A = np.zeros((n, n), dtype=np.float64)\n",
    "    B = np.zeros((n, n), dtype=np.float64)\n",
    "    \n",
    "    # Simulate output\n",
    "    for k in range(m):\n",
    "        startnode = startnodes[k]\n",
    "        endnode = endnodes[startnode]\n",
    "        while(indegree_t[startnode] == 0):\n",
    "            alpha_i = alpha[startnode]\n",
    "            beta_i = beta[startnode]\n",
    "            chi_i = chi[startnode]\n",
    "            gamma_i = gamma[startnode]\n",
    "            alpha_j = alpha[endnode]\n",
    "            beta_j = beta[endnode]\n",
    "            # System matrix\n",
    "            A[startnode, startnode] = chi_i\n",
    "            B[startnode, startnode] = gamma_i\n",
    "            # Add outflow to inflow at endnode\n",
    "            if startnode != endnode:\n",
    "                A[endnode, startnode] += beta_j\n",
    "                A[endnode] += alpha_j * A[startnode]\n",
    "                B[endnode] += alpha_j * B[startnode]\n",
    "            indegree_t[endnode] -= 1\n",
    "            startnode = endnode\n",
    "            endnode = endnodes[startnode]\n",
    "    return A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac641584-bf0b-4443-93e9-420b46cddb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../NYC_2022/new_year_creek_full.json') as nyc:\n",
    "    d = json.load(nyc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6009cd-0f88-4198-9223-d65c465e0058",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = [i['uid'] for i in d['nodes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705c329a-f540-4cd2-bbcd-8fefd1487587",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_ids = [i['uid'] for i in d['links']]\n",
    "reach_ids = [link_id[2:] for link_id in link_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73cd0b0-cd6b-46d4-8f2d-192b5cccff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_node_ids = [i['source_uid'] for i in d['links']]\n",
    "target_node_ids = [i['target_uid'] for i in d['links']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251bb940-bdfc-472d-9e1e-f21c22fb3e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "startnodes = np.asarray([node_ids.index(i) if i in node_ids\n",
    "                         else -1 for i in source_node_ids])\n",
    "endnodes = np.asarray([node_ids.index(i) if i in node_ids \n",
    "                       else -1 for i in target_node_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c291e03-68df-47ca-8556-6b6406380ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_loops = []\n",
    "\n",
    "for i in range(len(startnodes)):\n",
    "    if endnodes[i] == -1:\n",
    "        self_loops.append(i)\n",
    "        endnodes[i] = startnodes[i]\n",
    "        \n",
    "indegree = np.bincount(endnodes.ravel(), minlength=startnodes.size)\n",
    "\n",
    "for self_loop in self_loops:\n",
    "    indegree[self_loop] -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce968e7e-2362-4c93-8486-74c03e70794c",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_lengths = np.asarray([i['length'] for i in d['links']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b322abb1-024d-4825-9857-735193d460b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = startnodes.size\n",
    "\n",
    "#X = 0.29 * np.ones(n)\n",
    "#K = 1.3 * link_lengths\n",
    "X = 0.29 * np.ones(n)\n",
    "K = 3600. * np.ones(n)\n",
    "dt = 3600.\n",
    "alpha = (dt - 2 * K * X) / (2 * K * (1 - X) + dt)\n",
    "beta = (dt + 2 * K * X) / (2 * K * (1 - X) + dt)\n",
    "chi = (2 * K * (1 - X) - dt) / (2 * K * (1 - X) + dt)\n",
    "gamma = dt / (K * (1 - X) + dt / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7679056a-cce4-4633-a10d-f6ee6f06d4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "startnodes = startnodes[(indegree == 0)]\n",
    "A, B = muskingum_matrix(startnodes, endnodes, alpha, beta, chi, gamma, indegree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe313e7-436c-4954-9f77-b5727f942cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load forcing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ceb6b-9f6d-43cc-9efe-54104a86370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_range = pd.date_range('20230501', '20230523', freq='6H').strftime('%Y%m%d%H')[:-1]\n",
    "models = [f'medium_range_{i}' for i in range(1,8)]\n",
    "site = '5559104'\n",
    "site_index = [link[2:] for link in link_ids].index(site)\n",
    "priors = np.ones(len(models)) / len(models)\n",
    "priors = dict(zip(models, priors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784484be",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream1 = pd.read_csv('../nyc2022/scalar_USGS_08111085_flow_Learned_P1H_HIS.csv')\n",
    "\n",
    "obs = upstream1.set_index(pd.to_datetime(upstream1['Unnamed: 0']).values)\n",
    "obs = obs.loc['20230430':'20230601']\n",
    "upstream1_cms = obs['299705_00060'].tz_localize('US/Central') / 35.314666212661\n",
    "upstream1_cms = upstream1_cms.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream2 = pd.read_csv('../nyc2022/scalar_USGS_08111090_flow_Learned_P1H_HIS.csv')\n",
    "\n",
    "obs = upstream2.set_index(pd.to_datetime(upstream1['Unnamed: 0']).values)\n",
    "obs = obs.loc['20230430':'20230601']\n",
    "upstream2_cms = obs['300306_00060'].tz_localize('US/Central') / 35.314666212661\n",
    "upstream2_cms = upstream2_cms.tz_convert('UTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2b55c-8992-42b3-a208-d528379091b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 2 * np.eye(n)\n",
    "P = 1 * np.eye(n)\n",
    "R = 0.01 * np.eye(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "upstream1 = reach_ids.index('5559138')\n",
    "upstream2 = reach_ids.index('5559076')\n",
    "obs_point = reach_ids.index(site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed08f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((1, n))\n",
    "H[:,obs_point] = 1      # Observation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1c6512",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = pd.read_csv('08111110_discharge.txt', sep='\\t', skiprows=29, header=None)\n",
    "obs = obs.set_index(pd.to_datetime(obs[2]).values)\n",
    "obs = obs.loc['20230430':'20230601']\n",
    "obs_cms = 0.0283168 * obs[4].tz_localize('US/Central')\n",
    "obs_cms = obs_cms.tz_convert('UTC')\n",
    "obs_cms = obs_cms.resample('1h').interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa6c509",
   "metadata": {},
   "source": [
    "# No sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3105c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "uncertainty = {}\n",
    "\n",
    "for date_index, date in enumerate(date_range):\n",
    "    output[date] = {}\n",
    "    uncertainty[date] = {}\n",
    "    start_datetime = pd.to_datetime(date, format='%Y%m%d%H').tz_localize('UTC')\n",
    "\n",
    "    end_datetime = start_datetime + pd.Timedelta(hours=204)\n",
    "\n",
    "    for model in models:\n",
    "        qBucket = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_{model}_qBucket.csv')\n",
    "        qSfcLatRunoff = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_{model}_qSfcLatRunoff.csv')\n",
    "        streamflow = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_{model}_streamflow.csv')\n",
    "\n",
    "        qBucket_da = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_data_assimilation_qBucket.csv')\n",
    "        qSfcLatRunoff_da = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_data_assimilation_qSfcLatRunoff.csv')\n",
    "        streamflow_da = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_data_assimilation_streamflow.csv')\n",
    "\n",
    "        qBucket['time'] = pd.to_datetime(qBucket['time'])\n",
    "        qBucket = qBucket.set_index('time').tz_localize('UTC')\n",
    "        qBucket_da['time'] = pd.to_datetime(qBucket_da['time'])\n",
    "        qBucket_da = qBucket_da.set_index('time').tz_localize('UTC')\n",
    "\n",
    "        qSfcLatRunoff['time'] = pd.to_datetime(qSfcLatRunoff['time'])\n",
    "        qSfcLatRunoff = qSfcLatRunoff.set_index('time').tz_localize('UTC')\n",
    "        qSfcLatRunoff_da['time'] = pd.to_datetime(qSfcLatRunoff_da['time'])\n",
    "        qSfcLatRunoff_da = qSfcLatRunoff_da.set_index('time').tz_localize('UTC')\n",
    "\n",
    "        streamflow['time'] = pd.to_datetime(streamflow['time'])\n",
    "        streamflow = streamflow.set_index('time').tz_localize('UTC')\n",
    "        streamflow_da['time'] = pd.to_datetime(streamflow_da['time'])\n",
    "        streamflow_da = streamflow_da.set_index('time').tz_localize('UTC')\n",
    "        \n",
    "        T = len(qBucket)\n",
    "        assert T == len(qSfcLatRunoff)\n",
    "        #assert T == len(streamflow)\n",
    "        \n",
    "        if date_index == 0:\n",
    "            o_t_prev = streamflow_da[reach_ids].iloc[0].values\n",
    "            p_t_prev = qSfcLatRunoff_da[reach_ids].iloc[0].values + qBucket_da[reach_ids].iloc[0].values\n",
    "            #P_t_prev = P.copy()\n",
    "            P_t_prev = np.diag(o_t_prev) * 0.1\n",
    "        else:\n",
    "            try:\n",
    "                o_t_prev = output[date_range[date_index - 1]][model].loc[start_datetime].values\n",
    "                p_t_prev = qSfcLatRunoff_da[reach_ids].loc[start_datetime].values + qBucket_da[reach_ids].loc[start_datetime].values\n",
    "                P_t_prev = uncertainty[date_range[date_index - 1]][model][start_datetime]\n",
    "            except:\n",
    "                o_t_prev = output[date_range[date_index - 2]][model].loc[start_datetime].values\n",
    "                p_t_prev = qSfcLatRunoff_da[reach_ids].loc[start_datetime].values + qBucket_da[reach_ids].loc[start_datetime].values\n",
    "                P_t_prev = uncertainty[date_range[date_index - 2]][model][start_datetime]\n",
    "\n",
    "        O_t = {}\n",
    "        P_t = {}\n",
    "\n",
    "        # ### DA ###\n",
    "        # dz = z[start_datetime] - H @ o_t_prev\n",
    "        # K = P_t_prev @ H.T @ np.linalg.inv(H @ P_t_prev @ H.T + R)\n",
    "        # o_t_prev = o_t_prev + K @ dz\n",
    "        # P_t_prev = (np.eye(n) - K @ H) @ P_t_prev\n",
    "\n",
    "        O_t[start_datetime] = o_t_prev\n",
    "        P_t[start_datetime] = P_t_prev\n",
    "        \n",
    "        for t in range(T):\n",
    "            previous_datetime = start_datetime + datetime.timedelta(hours=t)\n",
    "            next_datetime = start_datetime + datetime.timedelta(hours=t+1)\n",
    "            o_t_next = A @ o_t_prev + B @ p_t_prev\n",
    "            Q = np.diag(o_t_next) * 0.1\n",
    "            P_t_next = A @ P_t_prev @ A.T + Q\n",
    "\n",
    "            p_t_prev = qSfcLatRunoff[reach_ids].loc[next_datetime].values + qBucket[reach_ids].loc[next_datetime].values\n",
    "\n",
    "            O_t[next_datetime] = o_t_next\n",
    "            o_t_prev = o_t_next\n",
    "            P_t[next_datetime] = P_t_next\n",
    "            P_t_prev = P_t_next\n",
    "    \n",
    "        O_t = pd.DataFrame.from_dict(O_t, orient='index')\n",
    "        O_t.columns = reach_ids\n",
    "                \n",
    "        output[date][model] = O_t\n",
    "        uncertainty[date][model] = P_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36e8676",
   "metadata": {},
   "source": [
    "# With sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dc8997",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = np.zeros((3, n))\n",
    "H[0,upstream1] = 1 \n",
    "H[1,upstream2] = 1 \n",
    "H[2,obs_point] = 1      # Observation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d0147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 2 * np.eye(n)\n",
    "P = 1 * np.eye(n)\n",
    "R = 0.01 * np.eye(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2547de",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.concat([upstream1_cms, upstream2_cms, obs_cms], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f250c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1652c0-31c6-42f6-b235-ba709e0e709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_up = {}\n",
    "uncertainty_up = {}\n",
    "K_gain = {}\n",
    "\n",
    "for date_index, date in enumerate(date_range):\n",
    "    output_up[date] = {}\n",
    "    uncertainty_up[date] = {}\n",
    "    K_gain[date] = {}\n",
    "    start_datetime = pd.to_datetime(date, format='%Y%m%d%H').tz_localize('UTC')\n",
    "\n",
    "    for model in models:\n",
    "        qBucket = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_{model}_qBucket.csv')\n",
    "        qSfcLatRunoff = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_{model}_qSfcLatRunoff.csv')\n",
    "        streamflow = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_{model}_streamflow.csv')\n",
    "\n",
    "        qBucket_da = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_data_assimilation_qBucket.csv')\n",
    "        qSfcLatRunoff_da = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_data_assimilation_qSfcLatRunoff.csv')\n",
    "        streamflow_da = pd.read_csv(f'../nyc2022/export_20230501-27/{date}/{date}_data_assimilation_streamflow.csv')\n",
    "\n",
    "        qBucket['time'] = pd.to_datetime(qBucket['time'])\n",
    "        qBucket = qBucket.set_index('time').tz_localize('UTC')\n",
    "        qBucket_da['time'] = pd.to_datetime(qBucket_da['time'])\n",
    "        qBucket_da = qBucket_da.set_index('time').tz_localize('UTC')\n",
    "\n",
    "        qSfcLatRunoff['time'] = pd.to_datetime(qSfcLatRunoff['time'])\n",
    "        qSfcLatRunoff = qSfcLatRunoff.set_index('time').tz_localize('UTC')\n",
    "        qSfcLatRunoff_da['time'] = pd.to_datetime(qSfcLatRunoff_da['time'])\n",
    "        qSfcLatRunoff_da = qSfcLatRunoff_da.set_index('time').tz_localize('UTC')\n",
    "\n",
    "        streamflow['time'] = pd.to_datetime(streamflow['time'])\n",
    "        streamflow = streamflow.set_index('time').tz_localize('UTC')\n",
    "        streamflow_da['time'] = pd.to_datetime(streamflow_da['time'])\n",
    "        streamflow_da = streamflow_da.set_index('time').tz_localize('UTC')\n",
    "        \n",
    "        T = len(qBucket)\n",
    "        assert T == len(qSfcLatRunoff)\n",
    "        #assert T == len(streamflow)\n",
    "        \n",
    "        if date_index == 0:\n",
    "            o_t_prev = streamflow_da[reach_ids].iloc[0].values\n",
    "            p_t_prev = qSfcLatRunoff_da[reach_ids].iloc[0].values + qBucket_da[reach_ids].iloc[0].values\n",
    "            #P_t_prev = P.copy()\n",
    "            P_t_prev = np.diag(o_t_prev) * 0.1\n",
    "        else:\n",
    "            try:\n",
    "                o_t_prev = output_up[date_range[date_index - 1]][model].loc[start_datetime].values\n",
    "                p_t_prev = qSfcLatRunoff_da[reach_ids].iloc[0].values + qBucket_da[reach_ids].iloc[0].values\n",
    "                P_t_prev = uncertainty_up[date_range[date_index - 1]][model][start_datetime]\n",
    "            except:\n",
    "                o_t_prev = output_up[date_range[date_index - 2]][model].loc[start_datetime].values\n",
    "                p_t_prev = qSfcLatRunoff_da[reach_ids].iloc[0].values + qBucket_da[reach_ids].iloc[0].values\n",
    "                P_t_prev = uncertainty_up[date_range[date_index - 2]][model][start_datetime]\n",
    "\n",
    "        O_t = {}\n",
    "        P_t = {}\n",
    "\n",
    "        ### DA ###\n",
    "        dz = z.loc[start_datetime].values - H @ o_t_prev\n",
    "        K = P_t_prev @ H.T @ np.linalg.inv(H @ P_t_prev @ H.T + R)\n",
    "        o_t_prev = o_t_prev + K @ dz\n",
    "        gain = K @ dz\n",
    "        P_t_prev = (np.eye(n) - K @ H) @ P_t_prev\n",
    "\n",
    "        O_t[start_datetime] = o_t_prev\n",
    "        P_t[start_datetime] = P_t_prev\n",
    "       \n",
    "        for t in range(T):\n",
    "            previous_datetime = start_datetime + datetime.timedelta(hours=t)\n",
    "            next_datetime = start_datetime + datetime.timedelta(hours=t+1)\n",
    "            o_t_next = A @ o_t_prev + B @ p_t_prev\n",
    "            Q = np.diag(o_t_next) * 0.1\n",
    "            P_t_next = A @ P_t_prev @ A.T + Q\n",
    "\n",
    "            p_t_prev = qSfcLatRunoff[reach_ids].loc[next_datetime].values + qBucket[reach_ids].loc[next_datetime].values\n",
    "\n",
    "            O_t[next_datetime] = o_t_next\n",
    "            o_t_prev = o_t_next\n",
    "            P_t[next_datetime] = P_t_next\n",
    "            P_t_prev = P_t_next\n",
    "    \n",
    "        O_t = pd.DataFrame.from_dict(O_t, orient='index')\n",
    "        O_t.columns = reach_ids\n",
    "                \n",
    "        output_up[date][model] = O_t\n",
    "        uncertainty_up[date][model] = P_t\n",
    "        K_gain[date][model] = gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55301d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_sensor = {}\n",
    "\n",
    "for date, data in output_up.items():\n",
    "    for model, df in data.items():\n",
    "        mean_sensor[date] = df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mean_sensor)\n",
    "df_transposed = df.transpose()\n",
    "df_transposed.to_csv('stremflow_with_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5aaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_without_sensor = {}\n",
    "\n",
    "for date, data in output.items():\n",
    "    for model, df in data.items():\n",
    "        mean_without_sensor[date] = df.iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ac16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(mean_sensor)\n",
    "df_transposed = df.transpose()\n",
    "df_transposed.to_csv('stremflow_without_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad51cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_sensor = {}\n",
    "\n",
    "for date in date_range:\n",
    "    std_sensor[date] = {}\n",
    "    for timestamp, cov in uncertainty_up[date][model].items():\n",
    "        std_sensor[date] = np.sqrt(np.diag(cov))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ed56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df = pd.DataFrame(std_sensor)\n",
    "std_transposed = std_df.transpose()\n",
    "std_transposed.columns = O_t.columns\n",
    "std_transposed.to_csv('std_with_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35560a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = {}\n",
    "\n",
    "for date in date_range:\n",
    "    std[date] = {}\n",
    "    for timestamp, cov in uncertainty[date][model].items():\n",
    "        std[date] = np.sqrt(np.diag(cov))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf349f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df = pd.DataFrame(std)\n",
    "std_transposed = std_df.transpose()\n",
    "std_transposed.columns = O_t.columns\n",
    "std_transposed.to_csv('std_without_sensor.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003a47c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "difference = {}\n",
    "\n",
    "for date in date_range:\n",
    "    difference[date] = std[date] - std_sensor[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17861de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.colors as mcolors\n",
    "import datetime\n",
    "streams = gpd.read_file('../NYC_2022/data/streams.shp')\n",
    "streams = streams.set_index(streams['ID'].astype(int))\n",
    "gage_shp = gpd.read_file('../NYC_2022/data/Gauge.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b01afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "streams['ID'] = streams['ID'].astype(int)\n",
    "streams = streams.set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a1b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_range:\n",
    "    streams_temp = streams.copy()\n",
    "\n",
    "    value = std[date]\n",
    "\n",
    "    index = O_t.columns.astype(int)\n",
    "    K_gain_series = pd.Series(value, index=pd.Index(index, dtype=int), name='K_gain_medium_range_5')\n",
    "    K_gain_df = K_gain_series.reset_index()\n",
    "    K_gain_df.columns = ['ID', 'K_gain_medium_range_5']\n",
    "\n",
    "    streams_temp = streams_temp.reset_index()\n",
    "    streams_temp = streams_temp.merge(K_gain_df, on='ID', how='left')\n",
    "\n",
    "    date_obj = datetime.datetime.strptime(date, '%Y%m%d%H')\n",
    "    readable_date = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_visible(False)\n",
    "\n",
    "    vmin = 0\n",
    "    vmax = 5\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=vmin, vmax=vmax, vcenter=2.5)\n",
    "\n",
    "    # Plotting\n",
    "    streams_temp.plot(ax=ax, column='K_gain_medium_range_5', legend=True, cmap='YlOrRd', norm=norm)\n",
    "    gage_shp.to_crs(streams_temp.crs).plot(ax=ax, color='darkgrey', zorder=2, markersize=100)\n",
    "    ax.text(0.1, 0.9, f'{readable_date}', transform=ax.transAxes)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'./std_nosensor/img_{date}.jpg', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdcf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_range:\n",
    "    streams_temp = streams.copy()\n",
    "\n",
    "    value = std_sensor[date]\n",
    "\n",
    "    index = O_t.columns.astype(int)\n",
    "    K_gain_series = pd.Series(value, index=pd.Index(index, dtype=int), name='K_gain_medium_range_5')\n",
    "    K_gain_df = K_gain_series.reset_index()\n",
    "    K_gain_df.columns = ['ID', 'K_gain_medium_range_5']\n",
    "\n",
    "    streams_temp = streams_temp.reset_index()\n",
    "    streams_temp = streams_temp.merge(K_gain_df, on='ID', how='left')\n",
    "\n",
    "    date_obj = datetime.datetime.strptime(date, '%Y%m%d%H')\n",
    "    readable_date = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_visible(False)\n",
    "\n",
    "    vmin = 0\n",
    "    vmax = 5\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=vmin, vmax=vmax, vcenter=2.5)\n",
    "\n",
    "    # Plotting\n",
    "    streams_temp.plot(ax=ax, column='K_gain_medium_range_5', legend=True, cmap='YlOrRd', norm=norm)\n",
    "    gage_shp.to_crs(streams_temp.crs).plot(ax=ax, color='green', zorder=2, markersize=100)\n",
    "    ax.text(0.1, 0.9, f'{readable_date}', transform=ax.transAxes)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'./std_withsensor/img_{date}.jpg', bbox_inches='tight', dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f96a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in date_range:\n",
    "    streams_temp = streams.copy()\n",
    "\n",
    "    value = difference[date]\n",
    "\n",
    "    index = O_t.columns.astype(int)\n",
    "    K_gain_series = pd.Series(value, index=pd.Index(index, dtype=int), name='K_gain_medium_range_5')\n",
    "    K_gain_df = K_gain_series.reset_index()\n",
    "    K_gain_df.columns = ['ID', 'K_gain_medium_range_5']\n",
    "\n",
    "    streams_temp = streams_temp.reset_index()\n",
    "    streams_temp = streams_temp.merge(K_gain_df, on='ID', how='left')\n",
    "\n",
    "    date_obj = datetime.datetime.strptime(date, '%Y%m%d%H')\n",
    "    readable_date = date_obj.strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.patch.set_visible(False)\n",
    "\n",
    "    vmin = 0\n",
    "    vmax = 1\n",
    "    norm = mcolors.TwoSlopeNorm(vmin=vmin, vmax=vmax, vcenter=0.5)\n",
    "\n",
    "    # Plotting\n",
    "    streams_temp.plot(ax=ax, column='K_gain_medium_range_5', legend=True, cmap='YlOrRd', norm=norm)\n",
    "    gage_shp.to_crs(streams_temp.crs).plot(ax=ax, color='darkgrey', zorder=2, markersize=100)\n",
    "    ax.text(0.1, 0.9, f'{readable_date}', transform=ax.transAxes)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Save the plot\n",
    "    plt.savefig(f'./std_diff/img_{date}.jpg', bbox_inches='tight', dpi=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
